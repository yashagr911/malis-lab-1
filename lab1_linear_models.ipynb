{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MALIS Lab Session 1 - Fall 2020\n",
    "## Due date: November 9 - 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group :**\n",
    "\n",
    "**Name SURNAME, Name SURNAME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is to practice with linear models for both regression and classification via simple experiments. \n",
    "\n",
    "#### Learning goals\n",
    "After this lab, you should be able to:\n",
    "1. Interpret the coefficent estimates produced by a linear model\n",
    "2. Be familiar with the use of polynomial and categorical features\n",
    "3. Be familiar with the building blocks of a pipeline to make building, fitting, and tracking models easier\n",
    "3. Be able to make an informed choice of model based on the data at hand\n",
    "4. Understand the key differences between nearest neighbor and linear models\n",
    "\n",
    "#### Instructions:\n",
    "Experiments should be made by groups of two students. Each group should produce a Jupyter Notebook with all their results and comments. \n",
    "\n",
    "Submit your complete notebook as an archive (tar -cf groupXnotebook.tar lab1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression\n",
    "In this part, we will be working with a dataset scraped by <a href=\"https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data\">Shubham Maurya</a>, which collects facts about players in the English Premier League as of 2017. His original goal was to establish if there was a relationship between a player's popularity and his market value, as estimated by transfermrkt.com.\n",
    "\n",
    "**Your goal is to fit a model able to predict a player's market value.**\n",
    "\n",
    "The dataset contains the following information:\n",
    "\n",
    "| **Field**   |     **Description**      |  \n",
    "|-------------|-------------|\n",
    "| name   |  Name of the player | \n",
    "| club   |  Club of the player |\n",
    "| age    | Age of the player |\n",
    "|position| The usual position on the pitch|\n",
    "|position_cat| 1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers|\n",
    "|market_value| As on transfermrkt.com on July 20th, 2017|\n",
    "|page_views| Average daily Wikipedia page views from September 1, 2016 to May 1, 2017|\n",
    "|fpl_value| Value in Fantasy Premier League as on July 20th, 2017|\n",
    "|fpl_sel| % of FPL players who have selected that player in their team|\n",
    "|fpl_points| FPL points accumulated over the previous season|\n",
    "|region| 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World|\n",
    "|nationality| Player's nationality|\n",
    "|new_foreign| Whether a new signing from a different league, for 2017/18 (till 20th July)|\n",
    "|age_cat| a categorical version of the Age feature|\n",
    "|club_id| a numerical version of the Club feature|\n",
    "|big_club| Whether one of the Top 6 clubs|\n",
    "|new_signing| Whether a new signing for 2017/18 (till 20th July)|\n",
    "\n",
    "**Step 1:** The very first step is to have a deeper look into the data:\n",
    "1. Using pandas extract a dataframe called *league_df* from the file *football_data.csv* which is inside the folder *data* (as done similarly in Lab_0)\n",
    "2. Print the result of the method  ```name_dataframe.d_types```, in this way you print out the data types associated to each of the fields in the table\n",
    "3. Run the method ```name_dataframe.head(N)``` to look at first N instances of the dataframe.\n",
    "4. Use the method ```name_dataframe.describe( )``` to generate descriptive statistics that summarize each field of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT write in this cell, do NOT modify this cell\n",
    "import numpy as np\n",
    "np.random.seed(11)\n",
    "print_solutions=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Using pandas extract a dataframe called league_df from the file football_data.csv which is inside the folder data \n",
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"data/football_data.csv\")\n",
    "league_df=pd.DataFrame(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name             object\n",
      "club             object\n",
      "age               int64\n",
      "position         object\n",
      "position_cat      int64\n",
      "market_value    float64\n",
      "page_views        int64\n",
      "fpl_value       float64\n",
      "fpl_sel          object\n",
      "fpl_points        int64\n",
      "region          float64\n",
      "nationality      object\n",
      "new_foreign       int64\n",
      "age_cat           int64\n",
      "club_id           int64\n",
      "big_club          int64\n",
      "new_signing       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2.Print the result of the method name_dataframe.d_types\n",
    "print(league_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name     club  age position  position_cat  market_value  \\\n",
      "0  Alexis Sanchez  Arsenal   28       LW             1          65.0   \n",
      "1      Mesut Ozil  Arsenal   28       AM             1          50.0   \n",
      "\n",
      "   page_views  fpl_value fpl_sel  fpl_points  region nationality  new_foreign  \\\n",
      "0        4329       12.0  17.10%         264     3.0       Chile            0   \n",
      "1        4395        9.5   5.60%         167     2.0     Germany            0   \n",
      "\n",
      "   age_cat  club_id  big_club  new_signing  \n",
      "0        4        1         1            0  \n",
      "1        4        1         1            0  \n"
     ]
    }
   ],
   "source": [
    "# 3.Run the method name_dataframe.head(N) to look at first N instances of the dataframe.\n",
    "print(league_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              age  position_cat  market_value   page_views   fpl_value  \\\n",
      "count  461.000000    461.000000    461.000000   461.000000  461.000000   \n",
      "mean    26.804772      2.180043     11.012039   763.776573    5.447939   \n",
      "std      3.961892      1.000061     12.257403   931.805757    1.346695   \n",
      "min     17.000000      1.000000      0.050000     3.000000    4.000000   \n",
      "25%     24.000000      1.000000      3.000000   220.000000    4.500000   \n",
      "50%     27.000000      2.000000      7.000000   460.000000    5.000000   \n",
      "75%     30.000000      3.000000     15.000000   896.000000    5.500000   \n",
      "max     38.000000      4.000000     75.000000  7664.000000   12.500000   \n",
      "\n",
      "       fpl_points      region  new_foreign     age_cat     club_id  \\\n",
      "count  461.000000  460.000000   461.000000  461.000000  461.000000   \n",
      "mean    57.314534    1.993478     0.034707    3.206074   10.334056   \n",
      "std     53.113811    0.957689     0.183236    1.279795    5.726475   \n",
      "min      0.000000    1.000000     0.000000    1.000000    1.000000   \n",
      "25%      5.000000    1.000000     0.000000    2.000000    6.000000   \n",
      "50%     51.000000    2.000000     0.000000    3.000000   10.000000   \n",
      "75%     94.000000    2.000000     0.000000    4.000000   15.000000   \n",
      "max    264.000000    4.000000     1.000000    6.000000   20.000000   \n",
      "\n",
      "         big_club  new_signing  \n",
      "count  461.000000   461.000000  \n",
      "mean     0.303688     0.145336  \n",
      "std      0.460349     0.352822  \n",
      "min      0.000000     0.000000  \n",
      "25%      0.000000     0.000000  \n",
      "50%      0.000000     0.000000  \n",
      "75%      1.000000     0.000000  \n",
      "max      1.000000     1.000000  \n"
     ]
    }
   ],
   "source": [
    "# 4.Use the method name_dataframe.describe( ) to generate descriptive statistics that summarize each field of the dataframe.\n",
    "print(league_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/step1_0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/step1_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 - Prepare the data:**\n",
    "We split our data into two sets: one data set for training and another one that we will use at the end to test our model.\n",
    "\n",
    "1. Import the function ```train_test_split``` from ```sklearn.model_selection```\n",
    "2. Split your *league_df* in **input_df** made of all features except *market_value*, and **output_df** made of the feature *market_value* (*Hint :* to copy the values of a dataframe to another use the method ```name_dataframe['name_attribute'].copy()```, to drop an attribute use ```name_dataframe.drop('name_attribute',axis=1)```)\n",
    "3. Use ```train_test_split``` with a *test_size*=0.25 (25 % of inputs became the test set) in following way to obtain a train set and a test set.\n",
    "\n",
    "    ```X_train, X_test, y_train, y_test = train_test_split(input_df, output_df, test_size=0.2)```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #Step 1\n",
    "output_df=league_df['market_value'].copy() #Step 2\n",
    "input_df=league_df.drop(['market_value'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_solutions==True :\n",
    "    %load ./solutions/step2.py\n",
    "    \n",
    "#You code shere\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_df, output_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a lot of features that can be used to build the model. We will start by using <code>age, fpl_value, big_club</code> and <code>page_views</code>. \n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{y} = W_0 + W_1\\ x_{age} + W_2\\ x_{fplavalue} + W_3\\ x_{bigclub} + W_4\\ \\sqrt{x_{pageviews}}\n",
    "\\end{equation}\n",
    "\n",
    "We need to use *numpy* library and\n",
    "\n",
    "<code>from sklearn.preprocessing import PolynomialFeatures<\\code>\n",
    "\n",
    "we define a function called *prepare_data(name_dataframe)* which does the following:\n",
    "1. extract the above features from the dataframe and assign to a variable\n",
    "2. apply the *np.sqrt( )* on the values of <code>page_views</code>\n",
    "3. transform our variable in numpy array <code>np.array(variable)</code>\n",
    "4. and then add a columns of ones (for the variable of $W_0$) to the dummy variable\n",
    "5. return our variable\n",
    "    \n",
    "We then apply our function to *X_train* and *X_test* to obtain *input_train1* and *input_test1* respectively\n",
    "Then we create also *output_train* and *output_test* moving to numpy array *y_train1* and *y_test1* respectively. Test data will be used later in the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def prepare_data(df):\n",
    "    variable = df[['age', 'fpl_value', 'big_club']].copy()\n",
    "    variable['sqrt_page_views'] = np.sqrt(df[['page_views']])\n",
    "    variable_array = np.array(variable)\n",
    "    variable_array = PolynomialFeatures(1).fit_transform(variable_array)\n",
    "\n",
    "    return variable_array\n",
    "\n",
    "input_train1 = prepare_data(X_train)\n",
    "input_test1 = prepare_data(X_test)\n",
    "output_train1 = np.array(y_train)\n",
    "output_test1 = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 - Training**\n",
    "\n",
    "Here we define a class called *my_linear_regression* which initializes with *__init__(self)* as empty the properties self.X_train, self.y_train and self.weights. Then we define the method *fit()* of the class which needs as parameters a matrix (X) and an array (y) with the outputs. With those parameters the class initializes self.x_train, self.y_train using the matrix and the array respectively and self.weigths using the formula of the linear regression: $$\\hat{W}=(𝑿^𝑇\\ 𝑿)^{−1}𝑿^𝑇\\ 𝒚$$\n",
    "\n",
    "We are using the function *np.linalg.solve*($𝑿^𝑇\\ 𝑿,𝑿^𝑇\\ 𝒚$) to obtain $\\hat{W}$ because it is more efficient than matrix inversion and it avoids numerical inestabilities.\n",
    "\n",
    "The last function in the object *my_linear_regression* is the function *predict()*. Given the weights obtained through fit, this method estimates new y values using the input matrix X_test: \n",
    "\n",
    "<code>self.y_hat=np.sum(X*self.weights,axis=1)<\\code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function also receives an array y with the true y values to compute the mean square error (MSE) using the formula : $$MSE = \\dfrac{1}{N}\\sum_{i=1}^{N}(y_i -\\hat{y}_i)^2$$, where $y_i$ is y_test[i]. \n",
    "    \n",
    "**Implement the MSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_linear_regression:\n",
    "    def __init__(self) : # initialize constructor for the object to assign the object its properties\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self, X, y) :\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.weights = np.linalg.solve(X.T@X,X.T@y)\n",
    "    \n",
    "    def predict(self,x_test,y_test) : # method of the object that can be used\n",
    "        self.y_hat=np.sum(x_test*self.weights,axis=1)\n",
    "        self.MSE= (y_test - self.y_hat)**2\n",
    "        self.MSE=np.sum(self.MSE)/np.size(self.MSE)\n",
    "        \n",
    "        return self.y_hat, self.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.11162814882497\n",
      "[-0.11391484  4.51149558  5.17434206  0.29202352]\n",
      "43.03370901982883\n",
      "[12.  2.]\n",
      "[16.61914125  1.60133531]\n"
     ]
    }
   ],
   "source": [
    "model_1=my_linear_regression()\n",
    "model_1.fit(input_train1,output_train1)\n",
    "print(model_1.weights[0])\n",
    "print(model_1.weights[1:])\n",
    "x,y=model_1.predict(input_train1,output_train1)\n",
    "print(y)\n",
    "print(output_train1[:2])\n",
    "print(x[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your class by creating an object called *model_1* and fit it using the training data *(input_train1, output_train1)*, then predict the *y_hat* using the the **training** (NOT test) inputs and the training targets. Finally print the output of your weights weight[0], weight[1:] and your MSE obtained to see the parameters of your model and its performance with training data. (*Hint :* *model_1.property_name*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_solutions==True :\n",
    "    %load ./solutions/step3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# absolute error in training MSE computation #############\n",
      "\n",
      "absolute error case 1 = 0.000000e+00 \n",
      "\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMPARISON OF YOUR MSE RESULT WITH CORRECT ONE\n",
    "# You should see an error smaller than 10^(-8)\n",
    "import check \n",
    "\n",
    "print(\"############# absolute error in training MSE computation #############\\n\")\n",
    "print('absolute error case 1 = %e \\n' %abs(model_1.MSE-check.MSE_train_correct_1))\n",
    "print(\"######################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated weights W (excluding $W_0$) are associated to 'age', 'fpl_value', 'big_club' and 'page_views'. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 1:** How do you interpret the values of each of these parameters? Based on this information, what can you say about the effect in a player's market value of his:\n",
    "    1. age?\n",
    "    2. number of page views?\n",
    "    3. fpl value?\n",
    "\n",
    "Which of these features seems to have the largest effect on a player's value?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 1:** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/question1.py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell, convert it into Markdown cell after load\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/question1.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including polynomial features\n",
    "A *scatter matrix* is a pair-wise scatter plot of several variables presented in a matrix format. It can be used to determine whether the variables are correlated and whether the correlation is positive or negative.\n",
    "\n",
    "From the scatter matrix below we can explore the relationship that each \"potential\" input variable has with the target variable: the market value. \n",
    "\n",
    "When looking at the correlation between age and market value (first column, last row), it does not seem to be linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "to_plot=pd.concat([X_test,y_test],axis=1)\n",
    "scatter_matrix(to_plot, figsize=(30,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [9.5, 6]\n",
    "plt.scatter(input_train1[:,1], output_train1)\n",
    "plt.scatter(input_test1[:,1], output_test1)\n",
    "plt.legend(['train', 'test'])\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('market value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore considering a more complex effect of the age by including a quadratic term: \n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{y} = w_0 + w_1* x_{age} + w_2* x_{fplvalue} + w_3*x_{bigclub} + w_4*\\sqrt{x_{pageviews}} + w_5*x_{age}^2\n",
    "\\end{equation}\n",
    "\n",
    "The function below prepares the data to fit to this new model.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 2:** Create a new function *prepare_new_data(name_dataframe)* as done before, but which includes also the quadratic term regarding the age this time. Then as done before create input_train_2, input_test_2, output_train_2 and output_test_2 and run your linear regression (you need only to run your class, not to define again the class) but initialize a new object. (E.g. *model_2=my_linear_regression(...)*). Again show the weights and the resulting MSE for the training data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/exercise2.py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON OF YOUR MSE RESULT WITH CORRECT ONE\n",
    "# You should see an error smaller than 10^(-8)\n",
    "\n",
    "print(\"############# absolute error in training MSE computation #############\\n\")\n",
    "print('absolute error case 2 = %e \\n' %abs(model_2.MSE-check.MSE_train_correct_2))\n",
    "print(\"######################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 2:** Based on your MSE and weights results, what can you say about adding this term $age^2$ to your model?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 2:** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including categorical features\n",
    "It is well known that the position where a football player plays has an impact in his market value. Midfielders and stikers tend to be more expensive. *Your goal now is to include this information in the model.*  \n",
    "\n",
    "As seen from the description, the player position is encoded as a numeric variable (1, 2, 3, 4). However, they represent categories and not values on their own. Categorical variables are commonly encoded under a scheme denoted 1-of-K encoding. This allows to convert a variable representing K different categories into K different binary values. Example: \n",
    "\n",
    "| **attacker**   |     **midfielder** | **defender** | **goalkeeper**      |  \n",
    "|-------------|-------------|-------------|-------------|\n",
    "| 1   |  0 | 0 | 0 |\n",
    "| 0   |  1 | 0 | 0 |\n",
    "| 0   |  0 | 1 | 0 |\n",
    "| 0   |  0 | 0 | 1 |\n",
    "\n",
    "\n",
    "**Look carefully below** how to implement 1-of-K encoding so that your model includes information about a player's position\n",
    "\\begin{equation}\n",
    " \\hat{y} = w_0 + w_1*x_{age} + w_2*x_{fplvalue} + w_3*x_{bigclub} + w_4*x_{pageviews} + w_5*x_{age}^2 + w_6*x_{attaker} + w_7*x_{midfielder} + w_8*x_{defender} +w_9*x_{goalkeeper}\n",
    "\\end{equation}    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_with_position(df):\n",
    "    variable = df[['age', 'fpl_value', 'big_club']].copy()\n",
    "    variable['sqrt_page_views'] = np.sqrt(df[['page_views']])\n",
    "    variable['square_age'] = np.power( df[['age']],2)\n",
    "    variable=variable.join(pd.get_dummies(df.position_cat, prefix='pos')) # get_dummies to create 1-of-K encoding, join to add the new columns\n",
    "    variable_array = np.array(variable)\n",
    "    variable_array = PolynomialFeatures(1).fit_transform(variable_array)\n",
    "    \n",
    "    return variable_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 3:** As done before create input_train3, input_test3, output_train3 and output_test3 but using this time the function *prepare_data_with_position(df)*  and run you linear regression (you need only to run it, not to define it) creating a new object (E.g. *model_3=my_linear_regression(...)*). Again show the weights and the resulting MSE for this new model with training data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/exercise3_0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON OF YOUR MSE RESULT WITH CORRECT ONE\n",
    "# You should see an error smaller than 10^(-8)\n",
    "\n",
    "print(\"############# absolute error in training MSE computation #############\\n\")\n",
    "print('absolute error case 3 = %e \\n' %abs(model_3.MSE-check.MSE_train_correct_3))\n",
    "print(\"######################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **print** the MSE for training in the 3 cases and then **test** your 3 models (use the predict function they have) with their respective test data and print the MSE for testing in the 3 cases.\n",
    "E.g. model_1.predict(input_test1,output_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/exercise3_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON OF YOUR MSE RESULTS WITH CORRECT ONES\n",
    "# You should see errorS smaller than 10^(-8)\n",
    "\n",
    "print(\"############# absolute error in test MSE computation #################\\n\")\n",
    "print('absolute error case 1 = %e \\n' %abs(model_1.MSE-check.MSE_test_correct_1))\n",
    "print('absolute error case 2 = %e \\n' %abs(model_2.MSE-check.MSE_test_correct_2))\n",
    "print('absolute error case 3 = %e \\n' %abs(model_3.MSE-check.MSE_test_correct_3))\n",
    "print(\"######################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 3:** Based on your results, what can you say about adding the player's position to your model?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 3:** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 4:** To deal with the apparent non-linear relationship between age and market value, \n",
    "<a href=\"https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data\">Shubham Maurya</a> suggests to use categorical features for the age. Using your implementation of the 1-of-K encoding, include the age categorical features into the model (create a new *prepare_data_with_position_and_age()* function with also the categorical age).\n",
    "\n",
    "It is up to you to decide if you will keep or not as before also $x_{age}$ and $x_{age}^2$ as non categorical features. Whichever your choice is, <u>justify clearly your answer</u>. \n",
    "    \n",
    "Run the model and print and compare now the training error (MSE) for the four models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/exercise4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON OF YOUR MSE RESULT WITH CORRECT ONE\n",
    "# You should see error smaller than 10^(-8)\n",
    "\n",
    "print(\"############# absolute error in training MSE computation #############\\n\")\n",
    "print('absolute error case 4 = %e \\n' %abs(model_4.MSE-check.MSE_train_correct_4))\n",
    "print(\"######################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 4:** Based on your MSE results, which model would you choose to predict a player's market value? Justify your answer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 4:** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4 - Prediction:** Now, use the models to predict the market value of the players in the respective test data and print the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/step4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON OF YOUR MSE RESULT WITH CORRECT ONE\n",
    "# You should see error smaller than 10^(-8)\n",
    "\n",
    "print(\"############# absolute error in test MSE computation #############\\n\")\n",
    "print('absolute error case 4 = %e \\n' %abs(model_4.MSE-check.MSE_test_correct_4))\n",
    "print(\"######################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 5:** How did your model do? Was it the best performing? Based on your intuition (as this topic has not been yet covered), try to explain the obtained performance (i.e. why some models do better than others)? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 5:** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Linear Models for Classification\n",
    "\n",
    "\n",
    "In this part, we will be working with the <a href=\"https://archive.ics.uci.edu/ml/datasets/heart+Disease\">Heart Disease dataset</a>, from the <a href=\"https://archive.ics.uci.edu/ml/index.php\">UCI Machine Learning Repository</a>. \n",
    "\n",
    "This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database that we will use is the only one that has been used by ML researchers to this date. The \"diagnosis\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Additionally, there is a condition field that labels subjects as healthy (H, which is equivalent to condition 0) or with a diagnosed disease (D, which gathers any subject with condition 1-4). \n",
    "\n",
    "Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n",
    "\n",
    "**The goal of this section is to build two classifiers: ona that can discriminate healthy from diseased subjects and a second one that can precisely identify the diagnosis of a subject.**\n",
    "\n",
    "The dataset contains the following information:\n",
    "\n",
    "| **Field**   |     **Description**      |  \n",
    "|-------------|-------------|\n",
    "| age         |  age in years |\n",
    "| sex         |  (1 = male; 0 = female) |\n",
    "|cp           |chest pain type |\n",
    "|trestbps     |resting blood pressure (in mm Hg on admission to the hospital)|\n",
    "|cholserum  |  cholestoral in mg/dl|\n",
    "|fbs        |(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)|\n",
    "|restecg | resting electrocardiographic results|\n",
    "|thalach |maximum heart rate achieved|\n",
    "|exang | exercise induced angina (1 = yes; 0 = no)|\n",
    "|oldpeak | ST depression induced by exercise relative to rest |\n",
    "|slope |the slope of the peak exercise ST segment |\n",
    "|ca | number of major vessels (0-3) colored by flourosopy |\n",
    "|thal | 3 = normal; 6 = fixed defect; 7 = reversable defect|\n",
    "| diagnosis | 0= normal, 1,2,3,4 disease conditions|\n",
    "| condition | H=Healthy D=Diseased |\n",
    "\n",
    "**Step 1:** As usual, we start by inspecting the dataset, split in training and test and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "heart_df_full=pd.read_csv('./data/processed.cleveland.data') # read_csv since the file contains commas to separate fields\n",
    "heart_df=heart_df_full[~(heart_df_full == '?').any(1)]\n",
    "\n",
    "print('FIELDS:\\n\\n',heart_df.dtypes)\n",
    "print('\\nValues in the dataframe\\n')\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into training and testing\n",
    "\n",
    "X_train, X_test, y_train_all, y_test_all=train_test_split(heart_df.drop(['condition','diagnosis'], axis=1), heart_df[['condition','diagnosis']], test_size=60)\n",
    "y_train=y_train_all['condition']\n",
    "y_test=y_test_all['condition']\n",
    "\n",
    "# We define the function to prepare the data (1-of-K encoding for some variables)\n",
    "\n",
    "def prepare_data(df):\n",
    "    X_cols = df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']].copy()\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.sex, prefix='sex'))\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.cp, prefix='cp'))\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.fbs, prefix='fbs'))\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.restecg, prefix='restecg'))\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.exang, prefix='exang'))\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.slope, prefix='slope'))\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.ca, prefix='ca'))\n",
    "    X_cols=X_cols.join(pd.get_dummies(df.thal, prefix='thal'))\n",
    "    \n",
    "    X = X_cols.values # from dataframe to numpy\n",
    "    \n",
    "    return X\n",
    "\n",
    "# We prepare the data\n",
    "\n",
    "input_train = prepare_data(X_train)\n",
    "input_test = prepare_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "**Question 6:** Which features have been encoded using 1-of-K encoding? Why? <u>justify your answer</u>. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 6:** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary classification\n",
    "We will first use the input data to train a binary classifier using Logistic Regression. \n",
    "This time, we will use the algorithm as implemented by the library <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">scikit-learn.</a>\n",
    "\n",
    "**Step 2 - Training** is just about using the function logistic regression with input_train and y_train. We will call the LogisticRegression object *LR_model_1*. we will use <code>max_iter=200</code> and <code>solver='liblinear'</code> as parameters when you use LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model_1 = LogisticRegression(solver='liblinear', max_iter=200)\n",
    "LR_model_1.fit(input_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*************** Estimated parameters: ***********************')\n",
    "print('[W_0] : ',LR_model_1.intercept_,'\\n[W] :', LR_model_1.coef_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 7:** Based on these results, what can you say about the incidence of age in having a heart disease? Of Cholesterol? <u>justify your answer</u>. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 7:** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 - Predict:** Similarly as we did with the linear regression part, the function below collects the steps needed to predict the output of new data and to then assess the performance. To measure the performance of a classifier we will use accuracy. We define accuracy as:\n",
    "\n",
    "\\begin{equation}\n",
    "accuracy = \\dfrac{\\#correct}{\\#samples}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\#correct$ denotes the number of correctly classified samples and $\\#samples$ the total number of samples under consideration.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 6:** Create the function *predict_and_test(LR_model, X, y)* to predict results given *input_test* and then compute accuracy counting the number of correct predictions knowing that *output_test* is the correct prediction. (*Hint :* LogisticRegression objects have their predict function, just call it.). Then use your function to compute the **training** accuracy of your model. Call the accuracy *accuracy_training*\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/part2_step3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON OF YOUR ACCURACY RESULT WITH CORRECT ONE\n",
    "# You should see error smaller than 10^(-8)\n",
    "import check\n",
    "\n",
    "print(\"############# absolute error in training accuracy computation#########\\n\")\n",
    "print('absolute error case 1 = %e \\n' %abs(accuracy_training - check.accuracy_training_correct_1))\n",
    "print(\"######################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 8:** Is your model capable of differentiating healthy subjects from patients? <u>justify your answer</u>. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Answer 8:**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression thresholds\n",
    "An interesting feature of logistic regression is that it hands back *probabilities* of a given case being 1 or 0, rather than just 1s and 0s. That allows to set different cutoffs for what counts as a 1. Let us have a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_thresholds(LR_model, X, y, thresholds):\n",
    "    '''\n",
    "    Computes accuracies by varying the cut-off threshold given a model, the input data and a range of thresholds. A ground truth needs to be provided\n",
    "    to estimate the accuracy\n",
    "    Works specifically for the Cleveland dataset.\n",
    "    '''\n",
    "    y_bool = (y == 'H')\n",
    "    accuracies = [(np.sum(y_bool ==  ~(LR_model.predict_proba(X)[:,0] >= th)) / len(y)) for th in thresholds]\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# generate a range of thresholds from 0 to 1.\n",
    "h=.02\n",
    "x_min=0.0\n",
    "x_max=1.0\n",
    "\n",
    "thresholds=np.arange(x_min, x_max, h) # set of thresholds from 0 to 1\n",
    "accuracies = explore_thresholds(LR_model_1, input_train, y_train, thresholds) # computed accuracies for each threshold\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [9.5, 6]\n",
    "plt.scatter(thresholds, accuracies)\n",
    "plt.xlabel('Cut-off threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training accuracy as a function of the cut-off threshold')\n",
    "plt.axvline(x=0.5,color='gray', linestyle='--') \n",
    "\n",
    "accuracies_test = explore_thresholds(LR_model_1, input_test, y_test, thresholds)\n",
    "plt.scatter(thresholds, accuracies_test)\n",
    "\n",
    "plt.legend(['threshold', 'training', 'test'])\n",
    "plt.show()\n",
    "\n",
    "print('Max training accuracy : ',np.max(accuracies),'\\nThreshold of maximum training accuracy: ', thresholds[np.argmax(accuracies)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed line denotes the threshold value used \"by default\". \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Question 9:** Based on this results, would you rather use your \"personalized\" threshold to predict future samples? If yes, which value? <u>justify your answer</u>. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Answer 9:**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace** <code>custom_t = 0.5</code> with your *personalized* threshold if you have one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you chose a specific threshold value to use as cut-off replace 0.5 with your chosen value\n",
    "\n",
    "custom_t = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('******************  Testing accuracy (identifying diseased condition) *********************')\n",
    "print('ACCURACY: ', predict_and_test(LR_model_1,input_test, y_test))\n",
    "print('*******************************************************************************************')\n",
    "\n",
    "#If you chose a specific threshold value to use as cut-off this will be displayed\n",
    "if custom_t != 0.5:\n",
    "    print('*********  Testing accuracy with custom cut-off (identifying diseased condition)***********')\n",
    "    print('ACCURACY: ',explore_thresholds(LR_model_1, input_test, y_test, [custom_t])[0])\n",
    "    print('*******************************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial classification\n",
    "Now, we want to know the precise diagnosis of a subject and not just to identify their overall condition (healthy or sick). For that purpose we will use a multinomial classifier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the multinomial targets\n",
    "\n",
    "y_train_m=y_train_all['diagnosis']\n",
    "y_test_m=y_test_all['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 7:** Use LogisticRegression to train the multinomial case. Call the object LR_model_m. Use tha parameters of before + <code>multi_class='auto'</code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/part2_exercise7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_training_m = predict_and_test(LR_model_m, input_train, y_train_m)\n",
    "\n",
    "print('*******************  Training accuracy (identifying diseased diagnosis) *******************\\n')\n",
    "print('ACCURACY: ', accuracy_training_m, '\\n')\n",
    "print('*******************************************************************************************\\n\\n\\n')\n",
    "\n",
    "print('******************* absolute error in training accuracy computation ***********************\\n')\n",
    "print('absolute error multinomial case = %e \\n' %abs(accuracy_training_m - check.accuracy_training_correct_m))\n",
    "print('*******************************************************************************************\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume there is a dummy model that outputs healthy (diagnosis=0) for every single input. The code below implements such a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_model(X):\n",
    "    '''\n",
    "    Returns a zero (no disease) no matter the input\n",
    "    '''\n",
    "    return np.zeros(len(X))\n",
    "\n",
    "accuracy_dummy=np.sum(dummy_model(input_test)==y_test_m)/len(y_test_m)\n",
    "accuracy_test = predict_and_test(LR_model_m, input_test, y_test_m)\n",
    "\n",
    "print('******************  Testing accuracy *********************')\n",
    "print('ACCURACY multinomial: %f' %accuracy_test)\n",
    "print('ACCURACY dummy: %f' %accuracy_dummy)\n",
    "print('**********************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Question 10:** Which model is better between dummy model and your multinomial model? What can you say about your multinomial model based on these results? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Answer 10:**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Comparing to a different family of methods\n",
    "\n",
    "In this last part of the lab, we are going to compare logistic regression to a different type of methods that we have not covered in the course and, more specifically, the k nearest neighbors algorithm (kNN).\n",
    "\n",
    "kNN is considered a non-parametric method given that it makes few assumptions about the form of the data distribution. This approach is *memory-based* as it requires no model to be fit. Nearest-neighbor methods use the observations from the training set closest in input space to $x$ to form $\\hat{y}$. It is based on the assumption that if a sample's features are similar to the ones of points of one particular class then it belongs to that class. These points are known as nearest neighbors.\n",
    "\n",
    "The specific case where $k=1$ is denoted the nearest neighbor algorithm. Here $\\hat{y}$ is assigned the value $y_{l}$ of the closest point $x_{l}$ to $x$ in the training data. This corresponds to a *Voronoi tessellation* of the training data. \n",
    "\n",
    "#### Algorithm\n",
    "Given a query point $\\mathbf{x}_0$ and a training set $T=(\\mathbf{x}_i, y_i)$, $i = 1,..., N$:<br>\n",
    "1- Compute the distance $d(\\mathbf{x}_0, \\mathbf{x}_i)$ between $\\mathbf{x}_0$ and all $\\mathbf{x}_i \\in T$.<br>\n",
    "2- Sort all $ \\mathbf{x}_i$ using $d(\\mathbf{x}_0, \\mathbf{x}_i)$ as sorting criterion. Sort for increasing distance<br>\n",
    "3- Select the first $K$ points. This points are denoted the k-neighborhoods of $\\mathbf{x}_0$, $N_k(\\mathbf{x}_0)$.<br>\n",
    "4- Assign $\\hat{y}(\\mathbf{x}_0$) based on majority voting:\n",
    "\\begin{equation}\n",
    "\\hat{y}(\\mathbf{x}_0) = \\arg\\max_{y} \\sum_{\\mathbf{x}_i \\in N_k} I(y = y_i)\n",
    "\\end{equation}\n",
    "\n",
    "An illustration of the algorithm is shown below:\n",
    "<center>\n",
    "<img src=\"data/knn.png\" width=\"200\" />\n",
    "\n",
    "The test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle). Source: <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">Wikipedia</a>.    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 8:** Your first task is to implement the K nearest neighbor algorithm by completing the code below. Use the Euclidean distance to measure the distance between two points.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance # you can select the euclidean distance\n",
    "from scipy import stats #\n",
    "\n",
    "class KNN:\n",
    "    '''\n",
    "    k nearest neighboors algorithm class\n",
    "    __init__() initialize the model\n",
    "    train() trains the model\n",
    "    predict() predict the class for a new point\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K):\n",
    "        '''\n",
    "        INPUT :\n",
    "        - K : is a natural number bigger than 0 \n",
    "        '''\n",
    "\n",
    "        \n",
    "        # empty initialization of X and y\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        # K is the parameter of the algorithm representing the number of neighborhoods\n",
    "        self.k = K\n",
    "        \n",
    "    def train(self,X,y):\n",
    "        '''\n",
    "        INPUT :\n",
    "        - X : is a 2D Nx2 numpy array containing the coordinates of points\n",
    "        - y : is a 1D Nx1 numpy array containing the labels for the corrisponding row of X\n",
    "        '''        \n",
    "        \n",
    "        self.X=X.copy() # copy your training points\n",
    "        self.y=y.copy()\n",
    "       \n",
    "    def predict(self,X_new):\n",
    "        '''\n",
    "        INPUT :\n",
    "        - X_new : is a Mx2 numpy array containing the coordinates of new points whose label has to be predicted\n",
    "        \n",
    "        OUTPUT :\n",
    "        - y_hat : is a Mx1 numpy array containing the predicted labels for the X_new points\n",
    "        ''' \n",
    "            \n",
    "        ######### YOUR CODE HERE - do not delete this line ################\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######## END OF YOUR CODE HERE - do not delete this line ##########\n",
    "\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_solutions==True :\n",
    "    %load ./solutions/part3_exercise8.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing your implementation\n",
    "To evaluate the algorithm we are going to use synthetic data coming from a Gaussian mixture model composed of 3 multivariate Gaussian distributions with different means and common covariance. Data is obtained by calling the function <code>gaussians()</code> from the <code>utils.py</code> file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets, model_selection\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# Generate the training points from the 3 gaussian distributions\n",
    "X, y = utils.gaussians()\n",
    "\n",
    "\n",
    "# Create a grid of testing points\n",
    "h=.02 # space in the grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# xx is the x-axis coordinate of the points in the test set\n",
    "# yy is the y-axis coordinate of the points in the test set\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "# X_test contains the test set inputs (xx,yy)\n",
    "X_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "# Your KNN implementation\n",
    "# Parameter K defining the KNN algorithm\n",
    "K=3\n",
    "# Create a model for the KNN with inputs the ones from the gaussians and test the model using the grid of testing points\n",
    "knn_model_yours = KNN(K)\n",
    "knn_model_yours.train(X, y)\n",
    "y_test_yours=knn_model_yours.predict(X_test)\n",
    "# Put the result into a color plot\n",
    "y_test_yours = y_test_yours.reshape(xx.shape)\n",
    "\n",
    "\n",
    "#Scikit-learn implementation\n",
    "knn_model_scikit = neighbors.KNeighborsClassifier(K, algorithm='kd_tree') #, weights=weights)\n",
    "knn_model_scikit.fit(X, y)\n",
    "y_hat_scikit = knn_model_scikit.predict(X_test)\n",
    "# Put the result into a color plot\n",
    "y_hat_scikit = y_hat_scikit.reshape(xx.shape)    \n",
    "\n",
    "\n",
    "# Compare results -> see the comparing_plots function in utils.py\n",
    "utils.comparing_plots(xx,yy, X, y, y_hat_scikit, y_test_yours, \"Scikit learn\", \"Your implementation with K=\" + str(K))\n",
    "\n",
    "# ATTENTION! the circles in the pictures are the training points.\n",
    "# Each training input can be red, blue or green depending on its label,\n",
    "# While the predicted labels for each point in the cartesian plane are the coloured areas, which means\n",
    "# all the red areas contain the points predicted as belonging to the red gaussian,\n",
    "# all the green areas contain the points predicted as belonging to the green gaussian,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing KNN and Logistic regression\n",
    "Finally, we will compare the behaviour of a linear model, such as logistic regression, with KNN. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 9:** Using code from Part 2, train a logistic regression model with the training data from the gaussian dataset (called *X* and *y* in previous cell) and then predict the unseen samples <code>X_test</code>. Store your results in a variable called <code>logistic_results</code>. This will be used to plot the regions defined by the decision functions of the logistic regressor.\n",
    "    \n",
    "Train your KNN classifier, with a K of your choice and then predict the unseen samples. Store the results in a variable called <code>knn_results</code>.\n",
    "    \n",
    "Plot your results.\n",
    "   \n",
    "Have a look at the plots:\n",
    "    \n",
    "    1. What can you say of the behaviour of the two models? \n",
    "    2. Play around with your K. What changes do you see? Which value of K would you recommend to get decision boundaries similar to those of logistic regression?\n",
    "    3. Based on your results, when would you recommend to use KNN? When a linear model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_solutions==True :\n",
    "    %load ./solutions/part3_exercise9.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
